{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Sequence to Sequence Modeling\n",
    "\n",
    "The aim of this homework is to familiarize you with sequence-to-sequence language modeling, specifically using an encoder-decoder model. This is the coding portion; you also have a written portion. The written portion can be found the homework instructions, i.e. the pdf you download from the syllabus website. In this notebook, you are provided with pre-written code for a simple sequence-to-sequence model that already works and learns how to reverse short sequences of numbers.\n",
    "\n",
    "If you run this whole jupyter notebook, it will learn to reverse short sequences of numbers. Although much of this code you will not be modifying, we recommend reading through it to get a sense of how the model and training works.\n",
    "\n",
    "This starter code is based on [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) by Sean Robertson from the PyTorch website. It has been modified by Katy Ilonka Gero for COMS W4705 taught at Columbia University by Professor Kathy McKeown. \n",
    "\n",
    "### Overview\n",
    "\n",
    "Your assignment is to:\n",
    "\n",
    "1. modify this code to run with the E2E restaurant data set (provided)\n",
    "2. train a model on this dataset on a GPU\n",
    "3. implement beam search for evaluation\n",
    "4. implement a BLEU evaluator and report BLEU scores\n",
    "\n",
    "These do not need to be done in this order.\n",
    "\n",
    "You must submit:\n",
    "\n",
    "1. This jupyter notebook, with your solutions to the above assignments. (Note cells that require specific outputs. Do not clear outputs before submitting.)\n",
    "2. A saved model (encoder and decoder) that takes in a meaning representation and generates a restaurant description.\n",
    "\n",
    "Write all your code **in this jupyter notebook**. Cells are provided where you should be implementing your code. See homework instructions for further details on how to submit this homework.\n",
    "\n",
    "### 1. Modify to work with E2E Dataset\n",
    "\n",
    "You will be working with the end-to-end (E2E) challenge dataset. More information can be found on [their website](http://www.macs.hw.ac.uk/InteractionLab/E2E/). In this dataset, the inputs are restaurant meaning representations, which are a series of key-value pairs that encode information about a restaurant. The outputs are fluent sentences that describe the restaurant. Here is an example:\n",
    "\n",
    "*Input: Meaning Representation*\n",
    "\n",
    "```\n",
    "name[The Eagle],\n",
    "eatType[coffee shop],\n",
    "food[French],\n",
    "priceRange[moderate],\n",
    "customerRating[3/5],\n",
    "area[riverside],\n",
    "kidsFriendly[yes],\n",
    "near[Burger King]\n",
    "```\n",
    "\n",
    "*Output: Fluent Sentence*\n",
    "\n",
    "```\n",
    "The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n",
    "```\n",
    "\n",
    "You will need to read in and process the training and development data. This data is provided in csv format. Here is an example line from the `trainset.csv` file:\n",
    "\n",
    "```\n",
    "\"name[Browns Cambridge], eatType[coffee shop], food[English], customer rating[5 out of 5], area[riverside], familyFriendly[no], near[Crowne Plaza Hotel]\",\"Browns Cambridge, a 5 out of 5 English coffee shop, is not kid friendly. It is located near Crowne Plaza Hotel and riverside.\"\n",
    "```\n",
    "\n",
    "You will need to tokenize the input and output. The input should be tokenized such that each token is a single entry from the meaning representation. You can decide how to tokenize the output. Here is how the input should be tokenize, and a simple tokenization for the output:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "```\n",
    "['name[Browns Cambridge]', 'eatType[coffee shop]', 'food[English]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Crowne Plaza Hotel]']\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "\n",
    "```\n",
    "['<SOS>', 'Browns', 'Cambridge,', 'a', '5', 'out', 'of', '5', 'English', 'coffee', 'shop,', 'is', 'not', 'kid', 'friendly.', 'It', 'is', 'located', 'near', 'Crowne', 'Plaza', 'Hotel', 'and', 'riverside.', '<EOS>']\n",
    "```\n",
    "\n",
    "Be sure to note the `<SOS>` (start-of-sequence) and `<EOS>` (end-of-sequence) tokens in the output. This is important and necessary! The decoder requires start and end tokens; the start token gives it an initial input to start generating, and the end token lets you know when to stop.\n",
    "\n",
    "Your first goal is to load in this data with [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), a library used to manage text datasets in pytorch. *You do not need to change anything in the model or training or evaluation.* All you need to do is load in the data similar to how the number-reversal data is loaded in.\n",
    "\n",
    "### 2. Train a model on this data\n",
    "\n",
    "To train a model on this data in a reasonable period of time, you will need to run this notebook on the Google Cloud VM with a GPU. [This tutorial](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52) gives a good explanation of how to use jupyter notebooks from a Google Cloud VM. It can take time to correctly get set up on a GPU, so don't leave this to the last minute. \n",
    "\n",
    "However, we *do* recommend testing your code by loading in a small amount of data (say, 5 examples,) and training on these. This should train quickly even without a GPU and the model should be able to almost memorize the data. This is generally good practice with generative networks -- ensure your model will memorize a small amount of data.\n",
    "\n",
    "With the full e2e dataset on a GPU, it should take around 20 minutes to train a single epoch. You should see decent results after a single epoch. Decent results are sentences with a few mistakes, but are mostly readable. You are encouraged to see what kind of improvements can be found with more training or different parameters.\n",
    "\n",
    "You do not need to modify any code to train the model, nor are you allowed to modify the model. You may modify the `trainIters` function, if you would like to improve how you track progress, or change parameters while training. For example, it can be useful to decrease the teacher-forcing ratio as training progresses.\n",
    "\n",
    "*You must submit a trained model. This model must be a GPU model. It is not reasonable to train this model with a CPU; part of the assignment is training it on a GPU.*\n",
    "\n",
    "*Note that the model is trained using single examples -- that is, it doesn't use batching. Batching is possible with seq2seq models, but for simplicity of reading the code we have not implemented it here.*\n",
    "\n",
    "### 3. Implement a beam search evaluator\n",
    "\n",
    "We provide you with an evaluation function that takes in an input sequence and generates an output sentence given a trained model. This evaluation function performs *greedy decoding* by taking the most likely token at each generation step. You are required to implement a beam search version of this evaluation function, that keeps track of the top *k* most likely sequences at each generation step, and then returns the top *k* best sequences with their associated probabilities.\n",
    "\n",
    "### 4. Implement a BLEU evaluator and report scores\n",
    "\n",
    "While loss and accuracy are good for tracking training progress, they don't tell us much about how well the model generates meaningful sentences. You need to implement a BLEU evaluation function that takes in an input/output pair and returns the BLEU score for how well the model predicts the output.\n",
    "\n",
    "You can find a formal description of how to calculate BLEU in the original paper, [BLEU: A Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). We reprodue this formal description for you in the homework instructions.\n",
    "\n",
    "When reporting these scores, use the *development dataset* provided. Report scores for greedy decoding and beam search (beam size=3). For beam search, use the top-scoring output sentence as the score for that datapoint.\n",
    "\n",
    "*You must implement your BLEU evaluator from scratch.* There exist python libraries that implement BLEU for you. Do not use these.\n",
    "\n",
    "### Don't forget the written portion!\n",
    "\n",
    "A series of open-ended questions about these tasks are required for the written portion of this homework. Please see the homework instructions for this, as well as instructions about how to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NO MODIFY\n",
    "\n",
    "# this is useful for checking if your code is successfully using the GPU\n",
    "\n",
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dummy number reversal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "train_path = 'data/toy_reverse/train/data.txt'\n",
    "dev_path = 'data/toy_reverse/dev/data.txt'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the e2e data\n",
    "\n",
    "Load in the E2E data similar to how the dummy number reversal dataset is loaded. That is, use the same `torchtext.data.Field` and `torchtext.data.TabularDataset` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR LOADING THE E2E DATA HERE\n",
    "train_path = 'data/e2e-dataset/trainset.csv'\n",
    "dev_path = 'data/e2e-dataset/devset.csv'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True,\n",
    "    tokenize=lambda seq: seq.split(', ')\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='csv',\n",
    "        skip_header=True,\n",
    "        fields=[(\"src\", src), (\"tgt\", tgt)],\n",
    "        filter_pred=len_filter      \n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='csv',\n",
    "        skip_header=True,\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the vocab and some example data points.\n",
    "\n",
    "*If you have loaded in the E2E dataset correctly, the code in the cell below should work without any modification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'familyFriendly[yes]', 'area[riverside]', 'eatType[coffee shop]', 'familyFriendly[no]', 'area[city centre]', 'eatType[pub]', 'food[Japanese]', 'food[Italian]', 'food[Fast food]', 'food[French]', 'priceRange[moderate]', 'priceRange[less than £20]', 'customer rating[average]', 'customer rating[low]', 'priceRange[high]', 'customer rating[5 out of 5]', 'priceRange[more than £30]', 'food[Indian]']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', 'is', '<eos>', '<sos>', 'a', 'The', 'the', 'in', 'near', 'of', 'and', 'food', 'customer', 'located', 'It', 'restaurant', 'has', 'coffee', 'price']\n",
      "\n",
      "num training examples: 42037\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['name[The Phoenix]', 'food[French]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[city centre]']\n",
      "tgt:\n",
      " ['<sos>', 'You', 'can', 'find', 'cheap', 'French', 'food', 'at', 'The', 'Phoenix,', 'located', 'at', 'the', 'city', 'centre.', 'Customers', 'rave', 'about', 'it,', 'giving', 'it', 'a', '5', 'out', 'of', '5.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "    \n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                \n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # note batch size of 1, just for simplicity\n",
    "    # DO NOT INCREASE THE BATCH SIZE\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=mydevice, repeat=False)\n",
    "    \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "            \n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            \n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "            \n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "        \n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "# changed hidden size to 150\n",
    "hidden_size = 150\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some guidelines for how much training to expect. Note that these *guidelines*; they are not exact.\n",
    "\n",
    "Only 1 epoch is needed for the number reversal dataset. This produces near-perfect results, and should take less than 5 minutes to run on a CPU.\n",
    "\n",
    "To memorize ~5 examples of the e2e dataset, ~100 epochs are needed (with a high teacher forcing ratio). This produces near-perfect results.\n",
    "\n",
    "To train on the full e2e dataset, only 1 epoch is needed to see decent outputs on the training data. More are required to increase fluency and see improvements on the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 epochs...\n",
      "step: 1000\t avg loss: 4.22\t time for 1000 steps: 0.79 min\n",
      "step: 2000\t avg loss: 3.56\t time for 1000 steps: 0.81 min\n",
      "step: 3000\t avg loss: 3.37\t time for 1000 steps: 0.80 min\n",
      "step: 4000\t avg loss: 3.13\t time for 1000 steps: 0.80 min\n",
      "step: 5000\t avg loss: 3.08\t time for 1000 steps: 0.81 min\n",
      "step: 6000\t avg loss: 2.91\t time for 1000 steps: 0.77 min\n",
      "step: 7000\t avg loss: 2.84\t time for 1000 steps: 0.58 min\n",
      "step: 8000\t avg loss: 2.84\t time for 1000 steps: 0.58 min\n",
      "step: 9000\t avg loss: 2.81\t time for 1000 steps: 0.56 min\n",
      "step: 10000\t avg loss: 2.67\t time for 1000 steps: 0.58 min\n",
      "step: 11000\t avg loss: 2.59\t time for 1000 steps: 0.58 min\n",
      "step: 12000\t avg loss: 2.66\t time for 1000 steps: 0.58 min\n",
      "step: 13000\t avg loss: 2.59\t time for 1000 steps: 0.58 min\n",
      "step: 14000\t avg loss: 2.66\t time for 1000 steps: 0.57 min\n",
      "step: 15000\t avg loss: 2.59\t time for 1000 steps: 0.56 min\n",
      "step: 16000\t avg loss: 2.45\t time for 1000 steps: 0.57 min\n",
      "step: 17000\t avg loss: 2.55\t time for 1000 steps: 0.58 min\n",
      "step: 18000\t avg loss: 2.51\t time for 1000 steps: 0.58 min\n",
      "step: 19000\t avg loss: 2.48\t time for 1000 steps: 0.57 min\n",
      "step: 20000\t avg loss: 2.52\t time for 1000 steps: 0.59 min\n",
      "step: 21000\t avg loss: 2.44\t time for 1000 steps: 0.58 min\n",
      "step: 22000\t avg loss: 2.41\t time for 1000 steps: 0.58 min\n",
      "step: 23000\t avg loss: 2.43\t time for 1000 steps: 0.56 min\n",
      "step: 24000\t avg loss: 2.38\t time for 1000 steps: 0.63 min\n",
      "step: 25000\t avg loss: 2.45\t time for 1000 steps: 0.62 min\n",
      "step: 26000\t avg loss: 2.44\t time for 1000 steps: 0.62 min\n",
      "step: 27000\t avg loss: 2.36\t time for 1000 steps: 0.62 min\n",
      "step: 28000\t avg loss: 2.44\t time for 1000 steps: 0.61 min\n",
      "step: 29000\t avg loss: 2.36\t time for 1000 steps: 0.62 min\n",
      "step: 30000\t avg loss: 2.40\t time for 1000 steps: 0.62 min\n",
      "step: 31000\t avg loss: 2.38\t time for 1000 steps: 0.63 min\n",
      "step: 32000\t avg loss: 2.37\t time for 1000 steps: 0.63 min\n",
      "step: 33000\t avg loss: 2.37\t time for 1000 steps: 0.63 min\n",
      "step: 34000\t avg loss: 2.38\t time for 1000 steps: 0.62 min\n",
      "step: 35000\t avg loss: 2.35\t time for 1000 steps: 0.62 min\n",
      "step: 36000\t avg loss: 2.33\t time for 1000 steps: 0.62 min\n",
      "step: 37000\t avg loss: 2.32\t time for 1000 steps: 0.62 min\n",
      "step: 38000\t avg loss: 2.37\t time for 1000 steps: 0.63 min\n",
      "step: 39000\t avg loss: 2.34\t time for 1000 steps: 0.62 min\n",
      "step: 40000\t avg loss: 2.32\t time for 1000 steps: 0.59 min\n",
      "step: 41000\t avg loss: 2.36\t time for 1000 steps: 0.57 min\n",
      "step: 42000\t avg loss: 2.36\t time for 1000 steps: 0.57 min\n",
      "End of epoch 0, avg loss 2.60\n",
      "step: 1000\t avg loss: 2.37\t time for 1000 steps: 0.57 min\n",
      "step: 2000\t avg loss: 2.27\t time for 1000 steps: 0.58 min\n",
      "step: 3000\t avg loss: 2.28\t time for 1000 steps: 0.57 min\n",
      "step: 4000\t avg loss: 2.27\t time for 1000 steps: 0.57 min\n",
      "step: 5000\t avg loss: 2.23\t time for 1000 steps: 0.58 min\n",
      "step: 6000\t avg loss: 2.28\t time for 1000 steps: 0.57 min\n",
      "step: 7000\t avg loss: 2.31\t time for 1000 steps: 0.57 min\n",
      "step: 8000\t avg loss: 2.27\t time for 1000 steps: 0.57 min\n",
      "step: 9000\t avg loss: 2.25\t time for 1000 steps: 0.58 min\n",
      "step: 10000\t avg loss: 2.27\t time for 1000 steps: 0.59 min\n",
      "step: 11000\t avg loss: 2.27\t time for 1000 steps: 0.58 min\n",
      "step: 12000\t avg loss: 2.23\t time for 1000 steps: 0.57 min\n",
      "step: 13000\t avg loss: 2.25\t time for 1000 steps: 0.57 min\n",
      "step: 14000\t avg loss: 2.19\t time for 1000 steps: 0.58 min\n",
      "step: 15000\t avg loss: 2.24\t time for 1000 steps: 0.58 min\n",
      "step: 16000\t avg loss: 2.23\t time for 1000 steps: 0.62 min\n",
      "step: 17000\t avg loss: 2.26\t time for 1000 steps: 0.62 min\n",
      "step: 18000\t avg loss: 2.26\t time for 1000 steps: 0.62 min\n",
      "step: 19000\t avg loss: 2.25\t time for 1000 steps: 0.61 min\n",
      "step: 20000\t avg loss: 2.25\t time for 1000 steps: 0.60 min\n",
      "step: 21000\t avg loss: 2.22\t time for 1000 steps: 0.61 min\n",
      "step: 22000\t avg loss: 2.23\t time for 1000 steps: 0.61 min\n",
      "step: 23000\t avg loss: 2.18\t time for 1000 steps: 0.61 min\n",
      "step: 24000\t avg loss: 2.19\t time for 1000 steps: 0.63 min\n",
      "step: 25000\t avg loss: 2.20\t time for 1000 steps: 0.62 min\n",
      "step: 26000\t avg loss: 2.21\t time for 1000 steps: 0.62 min\n",
      "step: 27000\t avg loss: 2.17\t time for 1000 steps: 0.60 min\n",
      "step: 28000\t avg loss: 2.17\t time for 1000 steps: 0.63 min\n",
      "step: 29000\t avg loss: 2.24\t time for 1000 steps: 0.63 min\n",
      "step: 30000\t avg loss: 2.20\t time for 1000 steps: 0.62 min\n",
      "step: 31000\t avg loss: 2.20\t time for 1000 steps: 0.61 min\n",
      "step: 32000\t avg loss: 2.17\t time for 1000 steps: 0.65 min\n",
      "step: 33000\t avg loss: 2.20\t time for 1000 steps: 0.62 min\n",
      "step: 34000\t avg loss: 2.17\t time for 1000 steps: 0.64 min\n",
      "step: 35000\t avg loss: 2.19\t time for 1000 steps: 0.64 min\n",
      "step: 36000\t avg loss: 2.26\t time for 1000 steps: 0.66 min\n",
      "step: 37000\t avg loss: 2.20\t time for 1000 steps: 0.65 min\n",
      "step: 38000\t avg loss: 2.24\t time for 1000 steps: 0.65 min\n",
      "step: 39000\t avg loss: 2.26\t time for 1000 steps: 0.68 min\n",
      "step: 40000\t avg loss: 2.13\t time for 1000 steps: 0.75 min\n",
      "step: 41000\t avg loss: 2.22\t time for 1000 steps: 0.64 min\n",
      "step: 42000\t avg loss: 2.19\t time for 1000 steps: 0.65 min\n",
      "End of epoch 1, avg loss 2.23\n",
      "step: 1000\t avg loss: 2.23\t time for 1000 steps: 0.64 min\n",
      "step: 2000\t avg loss: 2.21\t time for 1000 steps: 0.64 min\n",
      "step: 3000\t avg loss: 2.11\t time for 1000 steps: 0.63 min\n",
      "step: 4000\t avg loss: 2.13\t time for 1000 steps: 0.63 min\n",
      "step: 5000\t avg loss: 2.08\t time for 1000 steps: 0.60 min\n",
      "step: 6000\t avg loss: 2.22\t time for 1000 steps: 0.59 min\n",
      "step: 7000\t avg loss: 2.16\t time for 1000 steps: 0.58 min\n",
      "step: 8000\t avg loss: 2.20\t time for 1000 steps: 0.58 min\n",
      "step: 9000\t avg loss: 2.12\t time for 1000 steps: 0.59 min\n",
      "step: 10000\t avg loss: 2.07\t time for 1000 steps: 0.59 min\n",
      "step: 11000\t avg loss: 2.19\t time for 1000 steps: 0.58 min\n",
      "step: 12000\t avg loss: 2.15\t time for 1000 steps: 0.59 min\n",
      "step: 13000\t avg loss: 2.18\t time for 1000 steps: 0.58 min\n",
      "step: 14000\t avg loss: 2.15\t time for 1000 steps: 0.58 min\n",
      "step: 15000\t avg loss: 2.15\t time for 1000 steps: 0.58 min\n",
      "step: 16000\t avg loss: 2.11\t time for 1000 steps: 0.59 min\n",
      "step: 17000\t avg loss: 2.10\t time for 1000 steps: 0.59 min\n",
      "step: 18000\t avg loss: 2.14\t time for 1000 steps: 0.59 min\n",
      "step: 19000\t avg loss: 2.09\t time for 1000 steps: 0.60 min\n",
      "step: 20000\t avg loss: 2.13\t time for 1000 steps: 0.59 min\n",
      "step: 21000\t avg loss: 2.14\t time for 1000 steps: 0.59 min\n",
      "step: 22000\t avg loss: 2.18\t time for 1000 steps: 0.60 min\n",
      "step: 23000\t avg loss: 2.13\t time for 1000 steps: 0.59 min\n",
      "step: 24000\t avg loss: 2.09\t time for 1000 steps: 0.59 min\n",
      "step: 25000\t avg loss: 2.22\t time for 1000 steps: 0.59 min\n",
      "step: 26000\t avg loss: 2.15\t time for 1000 steps: 0.60 min\n",
      "step: 27000\t avg loss: 2.10\t time for 1000 steps: 0.59 min\n",
      "step: 28000\t avg loss: 2.17\t time for 1000 steps: 0.59 min\n",
      "step: 29000\t avg loss: 2.15\t time for 1000 steps: 0.58 min\n",
      "step: 30000\t avg loss: 2.14\t time for 1000 steps: 0.59 min\n",
      "step: 31000\t avg loss: 2.14\t time for 1000 steps: 0.59 min\n",
      "step: 32000\t avg loss: 2.19\t time for 1000 steps: 0.59 min\n",
      "step: 33000\t avg loss: 2.15\t time for 1000 steps: 0.58 min\n",
      "step: 34000\t avg loss: 2.16\t time for 1000 steps: 0.58 min\n",
      "step: 35000\t avg loss: 2.12\t time for 1000 steps: 0.59 min\n",
      "step: 36000\t avg loss: 2.11\t time for 1000 steps: 0.58 min\n",
      "step: 37000\t avg loss: 2.12\t time for 1000 steps: 0.59 min\n",
      "step: 38000\t avg loss: 2.17\t time for 1000 steps: 0.58 min\n",
      "step: 39000\t avg loss: 2.16\t time for 1000 steps: 0.58 min\n",
      "step: 40000\t avg loss: 2.16\t time for 1000 steps: 0.59 min\n",
      "step: 41000\t avg loss: 2.14\t time for 1000 steps: 0.58 min\n",
      "step: 42000\t avg loss: 2.22\t time for 1000 steps: 0.58 min\n",
      "End of epoch 2, avg loss 2.15\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "# but be sure that it prints some indication of how training is progressing\n",
    "# modified epochs to 3\n",
    "trainIters(encoder1, decoder1, 3, print_every=1000, teacher_forcing_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR SAVING YOUR MODEL HERE\n",
    "\n",
    "# We encourage you to confirm that you can load your trained model here also\n",
    "# torch.save(encoder1,'encoder.mdl')\n",
    "# torch.save(decoder1,'decoder.mdl')\n",
    "encoder1=torch.load('encoder.mdl')\n",
    "decoder1=torch.load('decoder.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            next_word = output_vocab.itos[topi.item()]\n",
    "            decoded_words.append(next_word)\n",
    "            if next_word == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement beam search evaluator\n",
    "\n",
    "Be sure to return all the output sequences (i.e. if the beam size is k, you should return k sequences) and their associated probabilities. You will need the associated probabilities to select the best performing sequence when calculating BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[The Eagle]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[yes]', 'near[Café Brazil]']\n",
      "<sos> Located in riverside near Café Brazil is The Cambridge Blue. It has a average customer rating and is not family friendly. <eos>\n",
      "0.777452510336171\n",
      "<sos> Located in riverside near Café Brazil is The Cambridge Blue. It has a average customer rating and is family friendly. <eos>\n",
      "0.7905004674738104\n",
      "<sos> Located in riverside near Café Brazil is The Cambridge Blue. It has a average customer rating and a family friendly atmosphere. <eos>\n",
      "0.8201707757037618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE FOR BEAM SEARCH HERE\n",
    "\n",
    "# The output of this cell should be an example input from the dev set, \n",
    "# and three outputs from a beam search evaluator.\n",
    "def beam_search(k, encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # record seq with negative log likelihood\n",
    "        decoded_words = [[list(),0.0] for i in range(k)]\n",
    "        new_step = [[list(),0.0] for i in range(k)]\n",
    "        # run first iteration, record k largest outputs\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(k)\n",
    "        next_inputs=[]\n",
    "        next_hiddens=[]\n",
    "        for i in range(k):\n",
    "            next_word = output_vocab.itos[topi[0][i].item()]\n",
    "            decoded_words[i][0].append(next_word)\n",
    "            decoded_words[i][1] -= topv[0][i].item()\n",
    "            next_inputs.append(topi[0][i].squeeze().detach())\n",
    "            next_hiddens.append(decoder_hidden)\n",
    "            \n",
    "        # run following iterations, choose highest k out of k*k\n",
    "        candidates=[]\n",
    "        hiddens=[[] for i in range(k)]\n",
    "        flag=0\n",
    "        for di in range(1,max_length):\n",
    "            # try each candidate seq\n",
    "            for i in range(k):\n",
    "                # if last token is EOS, simply add to list\n",
    "                if decoded_words[i][0][-1]== EOS_TOKEN:\n",
    "                    candidates.append([i])\n",
    "                    candidates[-1].append(decoded_words[i][1])\n",
    "                # else pass on to next decoder\n",
    "                else:\n",
    "                    flag=1\n",
    "                    decoder_output, decoder_hidden = decoder(next_inputs[i], next_hiddens[i])\n",
    "                    hiddens[i]=decoder_hidden\n",
    "                    topv, topi = decoder_output.data.topk(k)\n",
    "                    for c in range(k):\n",
    "                        candidates.append([i])\n",
    "                        candidates[-1].append((decoded_words[i][1]*di-topv[0][c].item())/(di+1))\n",
    "                        candidates[-1].append(topi[0][c])\n",
    "                        \n",
    "            if not flag:\n",
    "                break\n",
    "            candidates.sort(key=lambda x:x[1])\n",
    "            for i in range(k):\n",
    "                new_step[i]=deepcopy(decoded_words[candidates[i][0]])\n",
    "                if len(candidates[i])== 3:\n",
    "                    next_inputs[i]=candidates[i][-1].detach()\n",
    "                    next_hiddens[i]=hiddens[candidates[i][0]]                    \n",
    "                    new_step[i][0].append(output_vocab.itos[candidates[i][-1].item()])\n",
    "                    new_step[i][1]=candidates[i][1]\n",
    "            decoded_words=deepcopy(new_step)       \n",
    "            candidates=[]\n",
    "        return decoded_words\n",
    "\n",
    "# beam search evaluation on a devset data point\n",
    "# sequences are followed by their negative log likelihood\n",
    "for i in range(1):\n",
    "    item = random.choice(data_dev.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words = beam_search(3,encoder1, decoder1, seq)\n",
    "    for word in words:\n",
    "        print(' '.join(word[0]))\n",
    "        print(word[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at some generated sequences! This is the fun part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[Bibimbap House]', 'food[Chinese]', 'priceRange[high]', 'area[city centre]', 'near[Clare Hall]']\n",
      "<sos> Bibimbap House is a high priced Chinese food Hall in the city Clare in <eos>\n",
      "\n",
      "['name[The Dumpling Tree]', 'eatType[pub]', 'food[Italian]', 'familyFriendly[no]', 'near[The Portland Arms]']\n",
      "<sos> The Dumpling Tree is a pub that serves Italian food. The near The Portland Arms. <eos>\n",
      "\n",
      "['name[The Plough]', 'eatType[pub]', 'food[English]', 'priceRange[high]', 'familyFriendly[no]', 'near[Café Rouge]']\n",
      "<sos> The Plough is a high priced English pub near Café Rouge. It is not children friendly. <eos>\n",
      "\n",
      "['name[The Punter]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[£20-25]', 'customer rating[high]', 'familyFriendly[no]', 'near[Café Sicilia]']\n",
      "<sos> The Punter is a coffee shop that serves Indian food Café Sicilia. It is a Café Sicilia. It has a a range of £20-25. It has a high customer rating and is not <eos>\n",
      "\n",
      "['name[The Plough]', 'eatType[pub]', 'food[Fast food]', 'priceRange[less than £20]', 'familyFriendly[no]', 'near[Café Rouge]']\n",
      "<sos> The Plough is a pub that serves fast food near Café Rouge and is not family friendly. <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "# This selects 5 random datapoints from the training data and shows the generated sequence\n",
    "\n",
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement BLEU evaluator\n",
    "\n",
    "Remember that when calculating BLEU using beam search, select the top-scoring sequence output using the model probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing dev data, classify by src for BLEU reference\n",
    "category=dict()\n",
    "collection=dict()\n",
    "pre=None\n",
    "count=0\n",
    "data_dev.examples.sort(key=lambda x:x.src)\n",
    "for i,line in enumerate(data_dev.examples):\n",
    "    if line.src!=pre:\n",
    "        count+=1\n",
    "        collection[count]=[]\n",
    "        pre=line.src\n",
    "    category[i]=count\n",
    "    collection[count].append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4216494753297754\n",
      "0.46096348799649844\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE FOR THE BLEU EVALUATION HERE\n",
    "\n",
    "# The output of this cell should be the average BLEU score on the dev set\n",
    "# for greedy decoding AND for beam search decoding (beam size = 3)\n",
    "\n",
    "def bleu(instance, output):\n",
    "# params: instance - index of current input\n",
    "# output - candidate sentence generated by evaluation functions\n",
    "    # construct reference list, and lower case all tokens\n",
    "    reference=[]\n",
    "    ref_len=[]\n",
    "    output=[x.lower() for x in output]\n",
    "    for i in collection[category[instance]]:\n",
    "        reference.append(list(map(lambda x:x.lower(),(data_dev.examples[i].tgt))))\n",
    "        ref_len.append(len(data_dev.examples[i].tgt))\n",
    "    \n",
    "    # calculate Brevity Penalty\n",
    "    can_len=len(output)\n",
    "    closest=ref_len[0]\n",
    "    for i in ref_len:\n",
    "        if abs(i-can_len)<abs(closest-can_len):\n",
    "            closest=i\n",
    "        elif abs(i-can_len)==abs(closest-can_len) and (i-can_len)<0:\n",
    "            closest=i\n",
    "    if closest<can_len:\n",
    "        bre_pen=1\n",
    "    else:\n",
    "        bre_pen=math.exp(1-can_len/closest)\n",
    "    \n",
    "    # calculate modified precision\n",
    "    co_count=[0]*4\n",
    "    can_counts=collections.Counter()\n",
    "    for n in range(1,5):        \n",
    "        for i in range(can_len-n+ 1):\n",
    "            ngram = tuple(output[i:i+n])\n",
    "            can_counts[ngram] += 1\n",
    "    ngram_ct=[0]*len(can_counts)\n",
    "    ref_counts=collections.Counter()\n",
    "    for ref in reference:\n",
    "        for n in range(1,5):        \n",
    "            for i in range(len(ref)-n+ 1):\n",
    "                ngram = tuple(ref[i:i+n])\n",
    "                ref_counts[ngram] += 1\n",
    "        for num,(tk,ct) in enumerate(can_counts.items()):\n",
    "            if tk in ref_counts:\n",
    "                if ct<=ref_counts[tk]:\n",
    "                    temp=ct\n",
    "                else:\n",
    "                    temp=ref_counts[tk]\n",
    "                if temp>ngram_ct[num]:\n",
    "                    ngram_ct[num]=temp\n",
    "        ref_counts.clear()\n",
    "    for num,(tk,ct) in enumerate(can_counts.items()):\n",
    "        co_count[len(tk)-1]+=ngram_ct[num]\n",
    "    wsum=0\n",
    "    # calculated weighted sum until one pn=0\n",
    "    for i in range(4):\n",
    "        if co_count[i]==0:\n",
    "            break\n",
    "        wsum += math.log(co_count[i]/(can_len-i))/(i+1)\n",
    "    \n",
    "    bleu=bre_pen*math.exp(wsum)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "\n",
    "# run bleu for greedy and beam-search\n",
    "greedy_bleu=[]\n",
    "beam_bleu=[]\n",
    "for i,line in enumerate(data_dev.examples):\n",
    "    words = evaluate(encoder1, decoder1, line.src)\n",
    "    greedy_bleu.append(bleu(i,words))\n",
    "    words = beam_search(3,encoder1, decoder1, line.src)[0][0]\n",
    "    beam_bleu.append(bleu(i,words))\n",
    "\n",
    "print(sum(greedy_bleu)/len(greedy_bleu))\n",
    "print(sum(beam_bleu)/len(beam_bleu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[The Wrestlers]', 'customer rating[average]', 'familyFriendly[yes]']\n",
      "<sos> The Wrestlers is a family friendly restaurant that is average <eos>\n",
      "\n",
      "['name[Clowns]', 'eatType[coffee shop]', 'food[English]', 'customer rating[low]', 'area[riverside]', 'near[Clare Hall]']\n",
      "<sos> Clowns is a coffee shop coffee shop in the riverside near Clare Hall. It is <eos>\n",
      "\n",
      "['name[Cocum]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'familyFriendly[no]']\n",
      "<sos> Cocum is a coffee shop that serves Chinese food is and moderate price range. <eos>\n",
      "\n",
      "['name[The Eagle]', 'customer rating[3 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Café Brazil]']\n",
      "<sos> The Vaults is a Café a Café a a has a a out of <eos>\n",
      "\n",
      "['name[The Wrestlers]', 'customer rating[5 out of 5]', 'familyFriendly[yes]', 'near[The Sorrento]']\n",
      "<sos> The Cambridge is a family friendly restaurant that is a The The a 5 of <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This selects 5 random datapoints from the dev data for 2.2 error analysis\n",
    "\n",
    "for i in range(5):\n",
    "    item = random.choice(data_dev.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[The Wrestlers]', 'customer rating[3 out of 5]', 'familyFriendly[yes]']\n",
      "Beam search decoder:\n",
      "<sos> With a 3 out of 5 customer rating, The Wrestlers is a kid friendly restaurant. <eos>\n",
      "0.6702219458187327\n",
      "<sos> With a 3 out of 5 customer rating, The Wrestlers is a kid friendly restaurant that serves moderately priced food. <eos>\n",
      "0.7290968027981845\n",
      "<sos> With a 3 out of 5 customer rating, The Wrestlers is a kid friendly restaurant that serves food. <eos>\n",
      "0.7891547203063964\n",
      "\n",
      "Greedy decoder\n",
      "<sos> The Wrestlers is a kid friendly restaurant that is friendly has a <eos>\n",
      "\n",
      "['name[Clowns]', 'eatType[coffee shop]', 'food[Chinese]', 'customer rating[5 out of 5]', 'area[riverside]', 'near[Clare Hall]']\n",
      "Beam search decoder:\n",
      "<sos> Clowns a coffee shop that serves Chinese food is located near Clare Hall in the riverside. <eos>\n",
      "0.9490481482611762\n",
      "<sos> Clowns a coffee shop that serves Chinese food is located near Clare Hall in the riverside. It has a 5 out of <eos>\n",
      "0.9773677587509155\n",
      "<sos> Clowns a coffee shop that serves Chinese food is located near Clare Hall in the riverside. It has a 5 out of 5 <eos>\n",
      "0.9780165863037109\n",
      "\n",
      "Greedy decoder\n",
      "<sos> Clowns is a coffee shop that serves a 5 5 5 near Clare Hall in a 5 <eos>\n",
      "\n",
      "['name[The Eagle]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
      "Beam search decoder:\n",
      "<sos> The Eagle, a family friendly coffee shop that serves Chinese food. It is located near Burger King on the riverside. It has a low customer rating and a price range of less than £20. <eos>\n",
      "0.6392583847045898\n",
      "<sos> The Eagle, a family friendly coffee shop that serves Chinese food. It is located near Burger King on the riverside. It has a low customer rating and a price range of less <eos>\n",
      "0.6646318435668945\n",
      "<sos> The Eagle, a family friendly coffee shop that serves Chinese food. It is located near Burger King on the riverside. It has a low customer rating and a price range of less than <eos>\n",
      "0.6711665289742607\n",
      "\n",
      "Greedy decoder\n",
      "<sos> The Eagle is a coffee shop that serves Chinese food Burger King in the family friendly coffee shop in the low price range. It is <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This selects three datapoints from the dev data for 2.3 analysis\n",
    "\n",
    "for i in range(3):\n",
    "    item = random.choice(data_dev.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    print('Beam search decoder:')\n",
    "    words = beam_search(3,encoder1, decoder1, seq)\n",
    "    for word in words:\n",
    "        print(' '.join(word[0]))\n",
    "        print(word[1])\n",
    "    print()\n",
    "    print('Greedy decoder')\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[Unseen Name]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[moderate]', 'area[city centre]', 'familyFriendly[no]', 'near[Raja Indian Cuisine]']\n",
      "<sos> There is a moderately priced coffee shop called The Wrestlers that serves Indian food. It is located in the city centre near Raja Indian Cuisine. It is not kid friendly. <eos>\n"
     ]
    }
   ],
   "source": [
    "# This inputs an unseen restaurant name for 2.6 analysis\n",
    "\n",
    "for i in range(1):\n",
    "    item = random.choice(data_dev.examples)\n",
    "    seq = item.src\n",
    "    seq[0]='name[Unseen Name]'\n",
    "    print(seq)\n",
    "    words = beam_search(3,encoder1, decoder1, seq)\n",
    "    for word in words:\n",
    "        print(' '.join(word[0]))\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coms4705] *",
   "language": "python",
   "name": "conda-env-coms4705-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
